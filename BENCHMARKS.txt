Test 1:

20 clients, 10 request / client, HungryIris (1 sec burning)

ML_POOL: 2 workers - 116 sec
ML_POOL: 4 workers - 74 sec
ML_POOL: 6 workers - 70 sec
ML_POOL: 11 workers - 50 sec
SYNC: 222 sec


------------------------------------------------------------------------------
How much latency MLPool introduces? (examples/estimate_latency_mlpool_adds.py)

Scoring CPU intensive model 100 times synchronously (1 job at a time)

Direct scoring: 110 sec
Scoring on the pool (1 worker): 103 sec

The MLPool logic doesn't seem to introduce any overhead, it even seems to speed
things up slightly (interesting)
